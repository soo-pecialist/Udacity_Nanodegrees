{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Pipeline Preparation\n",
    "Follow the instructions below to help you create your ML pipeline.\n",
    "### 1. Import libraries and load data from database.\n",
    "- Import Python libraries\n",
    "- Load dataset from database with [`read_sql_table`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_sql_table.html)\n",
    "- Define feature and target variables X and Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/soohyeonkim/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/soohyeonkim/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/soohyeonkim/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import nltk\n",
    "nltk.download(['punkt', 'wordnet', 'stopwords'])\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>message</th>\n",
       "      <th>original</th>\n",
       "      <th>genre</th>\n",
       "      <th>related</th>\n",
       "      <th>request</th>\n",
       "      <th>offer</th>\n",
       "      <th>aid_related</th>\n",
       "      <th>medical_help</th>\n",
       "      <th>medical_products</th>\n",
       "      <th>...</th>\n",
       "      <th>aid_centers</th>\n",
       "      <th>other_infrastructure</th>\n",
       "      <th>weather_related</th>\n",
       "      <th>floods</th>\n",
       "      <th>storm</th>\n",
       "      <th>fire</th>\n",
       "      <th>earthquake</th>\n",
       "      <th>cold</th>\n",
       "      <th>other_weather</th>\n",
       "      <th>direct_report</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>Weather update - a cold front from Cuba that c...</td>\n",
       "      <td>Un front froid se retrouve sur Cuba ce matin. ...</td>\n",
       "      <td>direct</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>Is the Hurricane over or is it not over</td>\n",
       "      <td>Cyclone nan fini osinon li pa fini</td>\n",
       "      <td>direct</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>Looking for someone but no name</td>\n",
       "      <td>Patnm, di Maryani relem pou li banm nouvel li ...</td>\n",
       "      <td>direct</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>UN reports Leogane 80-90 destroyed. Only Hospi...</td>\n",
       "      <td>UN reports Leogane 80-90 destroyed. Only Hospi...</td>\n",
       "      <td>direct</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12</td>\n",
       "      <td>says: west side of Haiti, rest of the country ...</td>\n",
       "      <td>facade ouest d Haiti et le reste du pays aujou...</td>\n",
       "      <td>direct</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                            message  \\\n",
       "0   2  Weather update - a cold front from Cuba that c...   \n",
       "1   7            Is the Hurricane over or is it not over   \n",
       "2   8                    Looking for someone but no name   \n",
       "3   9  UN reports Leogane 80-90 destroyed. Only Hospi...   \n",
       "4  12  says: west side of Haiti, rest of the country ...   \n",
       "\n",
       "                                            original   genre  related  \\\n",
       "0  Un front froid se retrouve sur Cuba ce matin. ...  direct        1   \n",
       "1                 Cyclone nan fini osinon li pa fini  direct        1   \n",
       "2  Patnm, di Maryani relem pou li banm nouvel li ...  direct        1   \n",
       "3  UN reports Leogane 80-90 destroyed. Only Hospi...  direct        1   \n",
       "4  facade ouest d Haiti et le reste du pays aujou...  direct        1   \n",
       "\n",
       "   request  offer  aid_related  medical_help  medical_products      ...        \\\n",
       "0        0      0            0             0                 0      ...         \n",
       "1        0      0            1             0                 0      ...         \n",
       "2        0      0            0             0                 0      ...         \n",
       "3        1      0            1             0                 1      ...         \n",
       "4        0      0            0             0                 0      ...         \n",
       "\n",
       "   aid_centers  other_infrastructure  weather_related  floods  storm  fire  \\\n",
       "0            0                     0                0       0      0     0   \n",
       "1            0                     0                1       0      1     0   \n",
       "2            0                     0                0       0      0     0   \n",
       "3            0                     0                0       0      0     0   \n",
       "4            0                     0                0       0      0     0   \n",
       "\n",
       "   earthquake  cold  other_weather  direct_report  \n",
       "0           0     0              0              0  \n",
       "1           0     0              0              0  \n",
       "2           0     0              0              0  \n",
       "3           0     0              0              0  \n",
       "4           0     0              0              0  \n",
       "\n",
       "[5 rows x 40 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data from database\n",
    "engine = create_engine('sqlite:///FigureEight_ETL.db')\n",
    "df = pd.read_sql(\"FigureEight_ETL\", con=engine)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((26216,), (26216, 36))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Define feature and target variables X and Y\n",
    "X = df['message'].values\n",
    "## define classes names\n",
    "categories = df.columns[4:].values\n",
    "Y = df[categories].values\n",
    "X.shape, Y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Write a tokenization function to process your text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    Normalize, tokenize, lemmatize, clean texts\n",
    "    \n",
    "    > Parameters:\n",
    "    text: raw text\n",
    "    \n",
    "    > Returns:\n",
    "    clean_tokens: tokens that went through aformentioned procedures\n",
    "    \"\"\"\n",
    "    \n",
    "    text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text.lower())\n",
    "    tokens = word_tokenize(text)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    clean_tokens = []\n",
    "    for tok in tokens:\n",
    "        clean_tok = lemmatizer.lemmatize(tok).lower().strip()\n",
    "        clean_tokens.append(clean_tok)\n",
    "    return clean_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Build a machine learning pipeline\n",
    "This machine pipeline should take in the `message` column as input and output classification results on the other 36 categories in the dataset. You may find the [MultiOutputClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.multioutput.MultiOutputClassifier.html) helpful for predicting multiple target variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get stopwords ready\n",
    "stopwords = tokenize(\" \".join(stopwords.words('english')))\n",
    "stopwords.extend([str(i) for i in range(0, 1000)])\n",
    "stopwords.extend(['000'])\n",
    "\n",
    "## define random forest pipeline\n",
    "rfc_pipe = Pipeline([\n",
    "        ('vect', CountVectorizer(tokenizer=tokenize, stop_words=stopwords)),\n",
    "        ('tfidf', TfidfTransformer()),\n",
    "        ('rfc', MultiOutputClassifier(RandomForestClassifier(n_estimators=50, n_jobs=10, random_state=4999)))\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Train pipeline\n",
    "- Split data into train and test sets\n",
    "- Train pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 38s, sys: 1.94 s, total: 4min 40s\n",
      "Wall time: 40.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## spliting train & test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.25, random_state=1004)\n",
    "\n",
    "## fit pipeline classifier\n",
    "rfc_pipe.fit(X_train, y_train)\n",
    "y_pred = rfc_pipe.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Test your model\n",
    "Report the f1 score, precision and recall for each output category of the dataset. You can do this by iterating through the columns and calling sklearn's `classification_report` on each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           Precision      Recall    F1_score     support\n",
      "                 related        0.81        0.81        0.81     4998.00\n",
      "                 request        0.89        0.89        0.89     1116.00\n",
      "                   offer        1.00        1.00        1.00       27.00\n",
      "             aid_related        0.78        0.78        0.78     2679.00\n",
      "            medical_help        0.92        0.92        0.92      530.00\n",
      "        medical_products        0.95        0.95        0.95      324.00\n",
      "       search_and_rescue        0.97        0.97        0.97      181.00\n",
      "                security        0.98        0.98        0.98      120.00\n",
      "                military        0.97        0.97        0.97      210.00\n",
      "             child_alone        1.00        1.00        1.00        0.00\n",
      "                   water        0.96        0.96        0.96      399.00\n",
      "                    food        0.95        0.95        0.95      702.00\n",
      "                 shelter        0.94        0.94        0.94      532.00\n",
      "                clothing        0.98        0.98        0.98      120.00\n",
      "                   money        0.98        0.98        0.98      156.00\n",
      "          missing_people        0.99        0.99        0.99       79.00\n",
      "                refugees        0.97        0.97        0.97      219.00\n",
      "                   death        0.96        0.96        0.96      288.00\n",
      "               other_aid        0.87        0.87        0.87      851.00\n",
      "  infrastructure_related        0.94        0.94        0.94      419.00\n",
      "               transport        0.95        0.95        0.95      322.00\n",
      "               buildings        0.96        0.96        0.96      309.00\n",
      "             electricity        0.98        0.98        0.98      137.00\n",
      "                   tools        0.99        0.99        0.99       40.00\n",
      "               hospitals        0.99        0.99        0.99       71.00\n",
      "                   shops        0.99        0.99        0.99       35.00\n",
      "             aid_centers        0.99        0.99        0.99       63.00\n",
      "    other_infrastructure        0.96        0.96        0.96      279.00\n",
      "         weather_related        0.88        0.88        0.88     1813.00\n",
      "                  floods        0.95        0.95        0.95      544.00\n",
      "                   storm        0.94        0.94        0.94      589.00\n",
      "                    fire        0.99        0.99        0.99       73.00\n",
      "              earthquake        0.97        0.97        0.97      614.00\n",
      "                    cold        0.98        0.98        0.98      127.00\n",
      "           other_weather        0.95        0.95        0.95      342.00\n",
      "           direct_report        0.85        0.85        0.85     1289.00\n"
     ]
    }
   ],
   "source": [
    "def multioutput_classification_report(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    This is classification report for multioutput classifiers\n",
    "    \n",
    "    > Parameters:\n",
    "    y_true: true labels; numpy.ndarray\n",
    "    y_pred: predicted labels; numpy.ndarray\n",
    "    \n",
    "    > Returns: None\n",
    "    \"\"\"\n",
    "    \n",
    "    supports = y_true.sum(axis=0)\n",
    "    print(\"{:>24s}{:>12s}{:>12s}{:>12s}{:>12s}\".format('', 'Precision', 'Recall', 'F1_score', 'support'))\n",
    "    for i in range(0, y_true.shape[1]):\n",
    "        _ = precision_recall_fscore_support(y_true[:, i], y_pred[:, i], average='micro')\n",
    "        print(\"{:>24s}{:>12.2f}{:>12.2f}{:>12.2f}{:>12.2f}\".format(categories[i], _[0], _[1], _[2], supports[i]))\n",
    "        \n",
    "multioutput_classification_report(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Improve your model\n",
    "Use grid search to find better parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'memory': None,\n",
       " 'steps': [('vect',\n",
       "   CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), preprocessor=None,\n",
       "           stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'you', 're', 'you', 've', 'you', 'll', 'you', 'd', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'she', 's', 'her', 'hers', 'herself', 'it', 'it', 's', 'it', 'itself', 'they', 'them', '...', '987', '988', '989', '990', '991', '992', '993', '994', '995', '996', '997', '998', '999', '000'],\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=<function tokenize at 0x1a1c720ea0>, vocabulary=None)),\n",
       "  ('tfidf',\n",
       "   TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True)),\n",
       "  ('rfc',\n",
       "   MultiOutputClassifier(estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "               max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "               min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "               min_samples_leaf=1, min_samples_split=2,\n",
       "               min_weight_fraction_leaf=0.0, n_estimators=50, n_jobs=10,\n",
       "               oob_score=False, random_state=4999, verbose=0,\n",
       "               warm_start=False),\n",
       "              n_jobs=1))],\n",
       " 'vect': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), preprocessor=None,\n",
       "         stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'you', 're', 'you', 've', 'you', 'll', 'you', 'd', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'she', 's', 'her', 'hers', 'herself', 'it', 'it', 's', 'it', 'itself', 'they', 'them', '...', '987', '988', '989', '990', '991', '992', '993', '994', '995', '996', '997', '998', '999', '000'],\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=<function tokenize at 0x1a1c720ea0>, vocabulary=None),\n",
       " 'tfidf': TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True),\n",
       " 'rfc': MultiOutputClassifier(estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "             max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "             min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "             min_samples_leaf=1, min_samples_split=2,\n",
       "             min_weight_fraction_leaf=0.0, n_estimators=50, n_jobs=10,\n",
       "             oob_score=False, random_state=4999, verbose=0,\n",
       "             warm_start=False),\n",
       "            n_jobs=1),\n",
       " 'vect__analyzer': 'word',\n",
       " 'vect__binary': False,\n",
       " 'vect__decode_error': 'strict',\n",
       " 'vect__dtype': numpy.int64,\n",
       " 'vect__encoding': 'utf-8',\n",
       " 'vect__input': 'content',\n",
       " 'vect__lowercase': True,\n",
       " 'vect__max_df': 1.0,\n",
       " 'vect__max_features': None,\n",
       " 'vect__min_df': 1,\n",
       " 'vect__ngram_range': (1, 1),\n",
       " 'vect__preprocessor': None,\n",
       " 'vect__stop_words': ['i',\n",
       "  'me',\n",
       "  'my',\n",
       "  'myself',\n",
       "  'we',\n",
       "  'our',\n",
       "  'ours',\n",
       "  'ourselves',\n",
       "  'you',\n",
       "  'you',\n",
       "  're',\n",
       "  'you',\n",
       "  've',\n",
       "  'you',\n",
       "  'll',\n",
       "  'you',\n",
       "  'd',\n",
       "  'your',\n",
       "  'yours',\n",
       "  'yourself',\n",
       "  'yourselves',\n",
       "  'he',\n",
       "  'him',\n",
       "  'his',\n",
       "  'himself',\n",
       "  'she',\n",
       "  'she',\n",
       "  's',\n",
       "  'her',\n",
       "  'hers',\n",
       "  'herself',\n",
       "  'it',\n",
       "  'it',\n",
       "  's',\n",
       "  'it',\n",
       "  'itself',\n",
       "  'they',\n",
       "  'them',\n",
       "  'their',\n",
       "  'theirs',\n",
       "  'themselves',\n",
       "  'what',\n",
       "  'which',\n",
       "  'who',\n",
       "  'whom',\n",
       "  'this',\n",
       "  'that',\n",
       "  'that',\n",
       "  'll',\n",
       "  'these',\n",
       "  'those',\n",
       "  'am',\n",
       "  'is',\n",
       "  'are',\n",
       "  'wa',\n",
       "  'were',\n",
       "  'be',\n",
       "  'been',\n",
       "  'being',\n",
       "  'have',\n",
       "  'ha',\n",
       "  'had',\n",
       "  'having',\n",
       "  'do',\n",
       "  'doe',\n",
       "  'did',\n",
       "  'doing',\n",
       "  'a',\n",
       "  'an',\n",
       "  'the',\n",
       "  'and',\n",
       "  'but',\n",
       "  'if',\n",
       "  'or',\n",
       "  'because',\n",
       "  'a',\n",
       "  'until',\n",
       "  'while',\n",
       "  'of',\n",
       "  'at',\n",
       "  'by',\n",
       "  'for',\n",
       "  'with',\n",
       "  'about',\n",
       "  'against',\n",
       "  'between',\n",
       "  'into',\n",
       "  'through',\n",
       "  'during',\n",
       "  'before',\n",
       "  'after',\n",
       "  'above',\n",
       "  'below',\n",
       "  'to',\n",
       "  'from',\n",
       "  'up',\n",
       "  'down',\n",
       "  'in',\n",
       "  'out',\n",
       "  'on',\n",
       "  'off',\n",
       "  'over',\n",
       "  'under',\n",
       "  'again',\n",
       "  'further',\n",
       "  'then',\n",
       "  'once',\n",
       "  'here',\n",
       "  'there',\n",
       "  'when',\n",
       "  'where',\n",
       "  'why',\n",
       "  'how',\n",
       "  'all',\n",
       "  'any',\n",
       "  'both',\n",
       "  'each',\n",
       "  'few',\n",
       "  'more',\n",
       "  'most',\n",
       "  'other',\n",
       "  'some',\n",
       "  'such',\n",
       "  'no',\n",
       "  'nor',\n",
       "  'not',\n",
       "  'only',\n",
       "  'own',\n",
       "  'same',\n",
       "  'so',\n",
       "  'than',\n",
       "  'too',\n",
       "  'very',\n",
       "  's',\n",
       "  't',\n",
       "  'can',\n",
       "  'will',\n",
       "  'just',\n",
       "  'don',\n",
       "  'don',\n",
       "  't',\n",
       "  'should',\n",
       "  'should',\n",
       "  've',\n",
       "  'now',\n",
       "  'd',\n",
       "  'll',\n",
       "  'm',\n",
       "  'o',\n",
       "  're',\n",
       "  've',\n",
       "  'y',\n",
       "  'ain',\n",
       "  'aren',\n",
       "  'aren',\n",
       "  't',\n",
       "  'couldn',\n",
       "  'couldn',\n",
       "  't',\n",
       "  'didn',\n",
       "  'didn',\n",
       "  't',\n",
       "  'doesn',\n",
       "  'doesn',\n",
       "  't',\n",
       "  'hadn',\n",
       "  'hadn',\n",
       "  't',\n",
       "  'hasn',\n",
       "  'hasn',\n",
       "  't',\n",
       "  'haven',\n",
       "  'haven',\n",
       "  't',\n",
       "  'isn',\n",
       "  'isn',\n",
       "  't',\n",
       "  'ma',\n",
       "  'mightn',\n",
       "  'mightn',\n",
       "  't',\n",
       "  'mustn',\n",
       "  'mustn',\n",
       "  't',\n",
       "  'needn',\n",
       "  'needn',\n",
       "  't',\n",
       "  'shan',\n",
       "  'shan',\n",
       "  't',\n",
       "  'shouldn',\n",
       "  'shouldn',\n",
       "  't',\n",
       "  'wasn',\n",
       "  'wasn',\n",
       "  't',\n",
       "  'weren',\n",
       "  'weren',\n",
       "  't',\n",
       "  'won',\n",
       "  'won',\n",
       "  't',\n",
       "  'wouldn',\n",
       "  'wouldn',\n",
       "  't',\n",
       "  '0',\n",
       "  '1',\n",
       "  '2',\n",
       "  '3',\n",
       "  '4',\n",
       "  '5',\n",
       "  '6',\n",
       "  '7',\n",
       "  '8',\n",
       "  '9',\n",
       "  '10',\n",
       "  '11',\n",
       "  '12',\n",
       "  '13',\n",
       "  '14',\n",
       "  '15',\n",
       "  '16',\n",
       "  '17',\n",
       "  '18',\n",
       "  '19',\n",
       "  '20',\n",
       "  '21',\n",
       "  '22',\n",
       "  '23',\n",
       "  '24',\n",
       "  '25',\n",
       "  '26',\n",
       "  '27',\n",
       "  '28',\n",
       "  '29',\n",
       "  '30',\n",
       "  '31',\n",
       "  '32',\n",
       "  '33',\n",
       "  '34',\n",
       "  '35',\n",
       "  '36',\n",
       "  '37',\n",
       "  '38',\n",
       "  '39',\n",
       "  '40',\n",
       "  '41',\n",
       "  '42',\n",
       "  '43',\n",
       "  '44',\n",
       "  '45',\n",
       "  '46',\n",
       "  '47',\n",
       "  '48',\n",
       "  '49',\n",
       "  '50',\n",
       "  '51',\n",
       "  '52',\n",
       "  '53',\n",
       "  '54',\n",
       "  '55',\n",
       "  '56',\n",
       "  '57',\n",
       "  '58',\n",
       "  '59',\n",
       "  '60',\n",
       "  '61',\n",
       "  '62',\n",
       "  '63',\n",
       "  '64',\n",
       "  '65',\n",
       "  '66',\n",
       "  '67',\n",
       "  '68',\n",
       "  '69',\n",
       "  '70',\n",
       "  '71',\n",
       "  '72',\n",
       "  '73',\n",
       "  '74',\n",
       "  '75',\n",
       "  '76',\n",
       "  '77',\n",
       "  '78',\n",
       "  '79',\n",
       "  '80',\n",
       "  '81',\n",
       "  '82',\n",
       "  '83',\n",
       "  '84',\n",
       "  '85',\n",
       "  '86',\n",
       "  '87',\n",
       "  '88',\n",
       "  '89',\n",
       "  '90',\n",
       "  '91',\n",
       "  '92',\n",
       "  '93',\n",
       "  '94',\n",
       "  '95',\n",
       "  '96',\n",
       "  '97',\n",
       "  '98',\n",
       "  '99',\n",
       "  '100',\n",
       "  '101',\n",
       "  '102',\n",
       "  '103',\n",
       "  '104',\n",
       "  '105',\n",
       "  '106',\n",
       "  '107',\n",
       "  '108',\n",
       "  '109',\n",
       "  '110',\n",
       "  '111',\n",
       "  '112',\n",
       "  '113',\n",
       "  '114',\n",
       "  '115',\n",
       "  '116',\n",
       "  '117',\n",
       "  '118',\n",
       "  '119',\n",
       "  '120',\n",
       "  '121',\n",
       "  '122',\n",
       "  '123',\n",
       "  '124',\n",
       "  '125',\n",
       "  '126',\n",
       "  '127',\n",
       "  '128',\n",
       "  '129',\n",
       "  '130',\n",
       "  '131',\n",
       "  '132',\n",
       "  '133',\n",
       "  '134',\n",
       "  '135',\n",
       "  '136',\n",
       "  '137',\n",
       "  '138',\n",
       "  '139',\n",
       "  '140',\n",
       "  '141',\n",
       "  '142',\n",
       "  '143',\n",
       "  '144',\n",
       "  '145',\n",
       "  '146',\n",
       "  '147',\n",
       "  '148',\n",
       "  '149',\n",
       "  '150',\n",
       "  '151',\n",
       "  '152',\n",
       "  '153',\n",
       "  '154',\n",
       "  '155',\n",
       "  '156',\n",
       "  '157',\n",
       "  '158',\n",
       "  '159',\n",
       "  '160',\n",
       "  '161',\n",
       "  '162',\n",
       "  '163',\n",
       "  '164',\n",
       "  '165',\n",
       "  '166',\n",
       "  '167',\n",
       "  '168',\n",
       "  '169',\n",
       "  '170',\n",
       "  '171',\n",
       "  '172',\n",
       "  '173',\n",
       "  '174',\n",
       "  '175',\n",
       "  '176',\n",
       "  '177',\n",
       "  '178',\n",
       "  '179',\n",
       "  '180',\n",
       "  '181',\n",
       "  '182',\n",
       "  '183',\n",
       "  '184',\n",
       "  '185',\n",
       "  '186',\n",
       "  '187',\n",
       "  '188',\n",
       "  '189',\n",
       "  '190',\n",
       "  '191',\n",
       "  '192',\n",
       "  '193',\n",
       "  '194',\n",
       "  '195',\n",
       "  '196',\n",
       "  '197',\n",
       "  '198',\n",
       "  '199',\n",
       "  '200',\n",
       "  '201',\n",
       "  '202',\n",
       "  '203',\n",
       "  '204',\n",
       "  '205',\n",
       "  '206',\n",
       "  '207',\n",
       "  '208',\n",
       "  '209',\n",
       "  '210',\n",
       "  '211',\n",
       "  '212',\n",
       "  '213',\n",
       "  '214',\n",
       "  '215',\n",
       "  '216',\n",
       "  '217',\n",
       "  '218',\n",
       "  '219',\n",
       "  '220',\n",
       "  '221',\n",
       "  '222',\n",
       "  '223',\n",
       "  '224',\n",
       "  '225',\n",
       "  '226',\n",
       "  '227',\n",
       "  '228',\n",
       "  '229',\n",
       "  '230',\n",
       "  '231',\n",
       "  '232',\n",
       "  '233',\n",
       "  '234',\n",
       "  '235',\n",
       "  '236',\n",
       "  '237',\n",
       "  '238',\n",
       "  '239',\n",
       "  '240',\n",
       "  '241',\n",
       "  '242',\n",
       "  '243',\n",
       "  '244',\n",
       "  '245',\n",
       "  '246',\n",
       "  '247',\n",
       "  '248',\n",
       "  '249',\n",
       "  '250',\n",
       "  '251',\n",
       "  '252',\n",
       "  '253',\n",
       "  '254',\n",
       "  '255',\n",
       "  '256',\n",
       "  '257',\n",
       "  '258',\n",
       "  '259',\n",
       "  '260',\n",
       "  '261',\n",
       "  '262',\n",
       "  '263',\n",
       "  '264',\n",
       "  '265',\n",
       "  '266',\n",
       "  '267',\n",
       "  '268',\n",
       "  '269',\n",
       "  '270',\n",
       "  '271',\n",
       "  '272',\n",
       "  '273',\n",
       "  '274',\n",
       "  '275',\n",
       "  '276',\n",
       "  '277',\n",
       "  '278',\n",
       "  '279',\n",
       "  '280',\n",
       "  '281',\n",
       "  '282',\n",
       "  '283',\n",
       "  '284',\n",
       "  '285',\n",
       "  '286',\n",
       "  '287',\n",
       "  '288',\n",
       "  '289',\n",
       "  '290',\n",
       "  '291',\n",
       "  '292',\n",
       "  '293',\n",
       "  '294',\n",
       "  '295',\n",
       "  '296',\n",
       "  '297',\n",
       "  '298',\n",
       "  '299',\n",
       "  '300',\n",
       "  '301',\n",
       "  '302',\n",
       "  '303',\n",
       "  '304',\n",
       "  '305',\n",
       "  '306',\n",
       "  '307',\n",
       "  '308',\n",
       "  '309',\n",
       "  '310',\n",
       "  '311',\n",
       "  '312',\n",
       "  '313',\n",
       "  '314',\n",
       "  '315',\n",
       "  '316',\n",
       "  '317',\n",
       "  '318',\n",
       "  '319',\n",
       "  '320',\n",
       "  '321',\n",
       "  '322',\n",
       "  '323',\n",
       "  '324',\n",
       "  '325',\n",
       "  '326',\n",
       "  '327',\n",
       "  '328',\n",
       "  '329',\n",
       "  '330',\n",
       "  '331',\n",
       "  '332',\n",
       "  '333',\n",
       "  '334',\n",
       "  '335',\n",
       "  '336',\n",
       "  '337',\n",
       "  '338',\n",
       "  '339',\n",
       "  '340',\n",
       "  '341',\n",
       "  '342',\n",
       "  '343',\n",
       "  '344',\n",
       "  '345',\n",
       "  '346',\n",
       "  '347',\n",
       "  '348',\n",
       "  '349',\n",
       "  '350',\n",
       "  '351',\n",
       "  '352',\n",
       "  '353',\n",
       "  '354',\n",
       "  '355',\n",
       "  '356',\n",
       "  '357',\n",
       "  '358',\n",
       "  '359',\n",
       "  '360',\n",
       "  '361',\n",
       "  '362',\n",
       "  '363',\n",
       "  '364',\n",
       "  '365',\n",
       "  '366',\n",
       "  '367',\n",
       "  '368',\n",
       "  '369',\n",
       "  '370',\n",
       "  '371',\n",
       "  '372',\n",
       "  '373',\n",
       "  '374',\n",
       "  '375',\n",
       "  '376',\n",
       "  '377',\n",
       "  '378',\n",
       "  '379',\n",
       "  '380',\n",
       "  '381',\n",
       "  '382',\n",
       "  '383',\n",
       "  '384',\n",
       "  '385',\n",
       "  '386',\n",
       "  '387',\n",
       "  '388',\n",
       "  '389',\n",
       "  '390',\n",
       "  '391',\n",
       "  '392',\n",
       "  '393',\n",
       "  '394',\n",
       "  '395',\n",
       "  '396',\n",
       "  '397',\n",
       "  '398',\n",
       "  '399',\n",
       "  '400',\n",
       "  '401',\n",
       "  '402',\n",
       "  '403',\n",
       "  '404',\n",
       "  '405',\n",
       "  '406',\n",
       "  '407',\n",
       "  '408',\n",
       "  '409',\n",
       "  '410',\n",
       "  '411',\n",
       "  '412',\n",
       "  '413',\n",
       "  '414',\n",
       "  '415',\n",
       "  '416',\n",
       "  '417',\n",
       "  '418',\n",
       "  '419',\n",
       "  '420',\n",
       "  '421',\n",
       "  '422',\n",
       "  '423',\n",
       "  '424',\n",
       "  '425',\n",
       "  '426',\n",
       "  '427',\n",
       "  '428',\n",
       "  '429',\n",
       "  '430',\n",
       "  '431',\n",
       "  '432',\n",
       "  '433',\n",
       "  '434',\n",
       "  '435',\n",
       "  '436',\n",
       "  '437',\n",
       "  '438',\n",
       "  '439',\n",
       "  '440',\n",
       "  '441',\n",
       "  '442',\n",
       "  '443',\n",
       "  '444',\n",
       "  '445',\n",
       "  '446',\n",
       "  '447',\n",
       "  '448',\n",
       "  '449',\n",
       "  '450',\n",
       "  '451',\n",
       "  '452',\n",
       "  '453',\n",
       "  '454',\n",
       "  '455',\n",
       "  '456',\n",
       "  '457',\n",
       "  '458',\n",
       "  '459',\n",
       "  '460',\n",
       "  '461',\n",
       "  '462',\n",
       "  '463',\n",
       "  '464',\n",
       "  '465',\n",
       "  '466',\n",
       "  '467',\n",
       "  '468',\n",
       "  '469',\n",
       "  '470',\n",
       "  '471',\n",
       "  '472',\n",
       "  '473',\n",
       "  '474',\n",
       "  '475',\n",
       "  '476',\n",
       "  '477',\n",
       "  '478',\n",
       "  '479',\n",
       "  '480',\n",
       "  '481',\n",
       "  '482',\n",
       "  '483',\n",
       "  '484',\n",
       "  '485',\n",
       "  '486',\n",
       "  '487',\n",
       "  '488',\n",
       "  '489',\n",
       "  '490',\n",
       "  '491',\n",
       "  '492',\n",
       "  '493',\n",
       "  '494',\n",
       "  '495',\n",
       "  '496',\n",
       "  '497',\n",
       "  '498',\n",
       "  '499',\n",
       "  '500',\n",
       "  '501',\n",
       "  '502',\n",
       "  '503',\n",
       "  '504',\n",
       "  '505',\n",
       "  '506',\n",
       "  '507',\n",
       "  '508',\n",
       "  '509',\n",
       "  '510',\n",
       "  '511',\n",
       "  '512',\n",
       "  '513',\n",
       "  '514',\n",
       "  '515',\n",
       "  '516',\n",
       "  '517',\n",
       "  '518',\n",
       "  '519',\n",
       "  '520',\n",
       "  '521',\n",
       "  '522',\n",
       "  '523',\n",
       "  '524',\n",
       "  '525',\n",
       "  '526',\n",
       "  '527',\n",
       "  '528',\n",
       "  '529',\n",
       "  '530',\n",
       "  '531',\n",
       "  '532',\n",
       "  '533',\n",
       "  '534',\n",
       "  '535',\n",
       "  '536',\n",
       "  '537',\n",
       "  '538',\n",
       "  '539',\n",
       "  '540',\n",
       "  '541',\n",
       "  '542',\n",
       "  '543',\n",
       "  '544',\n",
       "  '545',\n",
       "  '546',\n",
       "  '547',\n",
       "  '548',\n",
       "  '549',\n",
       "  '550',\n",
       "  '551',\n",
       "  '552',\n",
       "  '553',\n",
       "  '554',\n",
       "  '555',\n",
       "  '556',\n",
       "  '557',\n",
       "  '558',\n",
       "  '559',\n",
       "  '560',\n",
       "  '561',\n",
       "  '562',\n",
       "  '563',\n",
       "  '564',\n",
       "  '565',\n",
       "  '566',\n",
       "  '567',\n",
       "  '568',\n",
       "  '569',\n",
       "  '570',\n",
       "  '571',\n",
       "  '572',\n",
       "  '573',\n",
       "  '574',\n",
       "  '575',\n",
       "  '576',\n",
       "  '577',\n",
       "  '578',\n",
       "  '579',\n",
       "  '580',\n",
       "  '581',\n",
       "  '582',\n",
       "  '583',\n",
       "  '584',\n",
       "  '585',\n",
       "  '586',\n",
       "  '587',\n",
       "  '588',\n",
       "  '589',\n",
       "  '590',\n",
       "  '591',\n",
       "  '592',\n",
       "  '593',\n",
       "  '594',\n",
       "  '595',\n",
       "  '596',\n",
       "  '597',\n",
       "  '598',\n",
       "  '599',\n",
       "  '600',\n",
       "  '601',\n",
       "  '602',\n",
       "  '603',\n",
       "  '604',\n",
       "  '605',\n",
       "  '606',\n",
       "  '607',\n",
       "  '608',\n",
       "  '609',\n",
       "  '610',\n",
       "  '611',\n",
       "  '612',\n",
       "  '613',\n",
       "  '614',\n",
       "  '615',\n",
       "  '616',\n",
       "  '617',\n",
       "  '618',\n",
       "  '619',\n",
       "  '620',\n",
       "  '621',\n",
       "  '622',\n",
       "  '623',\n",
       "  '624',\n",
       "  '625',\n",
       "  '626',\n",
       "  '627',\n",
       "  '628',\n",
       "  '629',\n",
       "  '630',\n",
       "  '631',\n",
       "  '632',\n",
       "  '633',\n",
       "  '634',\n",
       "  '635',\n",
       "  '636',\n",
       "  '637',\n",
       "  '638',\n",
       "  '639',\n",
       "  '640',\n",
       "  '641',\n",
       "  '642',\n",
       "  '643',\n",
       "  '644',\n",
       "  '645',\n",
       "  '646',\n",
       "  '647',\n",
       "  '648',\n",
       "  '649',\n",
       "  '650',\n",
       "  '651',\n",
       "  '652',\n",
       "  '653',\n",
       "  '654',\n",
       "  '655',\n",
       "  '656',\n",
       "  '657',\n",
       "  '658',\n",
       "  '659',\n",
       "  '660',\n",
       "  '661',\n",
       "  '662',\n",
       "  '663',\n",
       "  '664',\n",
       "  '665',\n",
       "  '666',\n",
       "  '667',\n",
       "  '668',\n",
       "  '669',\n",
       "  '670',\n",
       "  '671',\n",
       "  '672',\n",
       "  '673',\n",
       "  '674',\n",
       "  '675',\n",
       "  '676',\n",
       "  '677',\n",
       "  '678',\n",
       "  '679',\n",
       "  '680',\n",
       "  '681',\n",
       "  '682',\n",
       "  '683',\n",
       "  '684',\n",
       "  '685',\n",
       "  '686',\n",
       "  '687',\n",
       "  '688',\n",
       "  '689',\n",
       "  '690',\n",
       "  '691',\n",
       "  '692',\n",
       "  '693',\n",
       "  '694',\n",
       "  '695',\n",
       "  '696',\n",
       "  '697',\n",
       "  '698',\n",
       "  '699',\n",
       "  '700',\n",
       "  '701',\n",
       "  '702',\n",
       "  '703',\n",
       "  '704',\n",
       "  '705',\n",
       "  '706',\n",
       "  '707',\n",
       "  '708',\n",
       "  '709',\n",
       "  '710',\n",
       "  '711',\n",
       "  '712',\n",
       "  '713',\n",
       "  '714',\n",
       "  '715',\n",
       "  '716',\n",
       "  '717',\n",
       "  '718',\n",
       "  '719',\n",
       "  '720',\n",
       "  '721',\n",
       "  '722',\n",
       "  '723',\n",
       "  '724',\n",
       "  '725',\n",
       "  '726',\n",
       "  '727',\n",
       "  '728',\n",
       "  '729',\n",
       "  '730',\n",
       "  '731',\n",
       "  '732',\n",
       "  '733',\n",
       "  '734',\n",
       "  '735',\n",
       "  '736',\n",
       "  '737',\n",
       "  '738',\n",
       "  '739',\n",
       "  '740',\n",
       "  '741',\n",
       "  '742',\n",
       "  '743',\n",
       "  '744',\n",
       "  '745',\n",
       "  '746',\n",
       "  '747',\n",
       "  '748',\n",
       "  '749',\n",
       "  '750',\n",
       "  '751',\n",
       "  '752',\n",
       "  '753',\n",
       "  '754',\n",
       "  '755',\n",
       "  '756',\n",
       "  '757',\n",
       "  '758',\n",
       "  '759',\n",
       "  '760',\n",
       "  '761',\n",
       "  '762',\n",
       "  '763',\n",
       "  '764',\n",
       "  '765',\n",
       "  '766',\n",
       "  '767',\n",
       "  '768',\n",
       "  '769',\n",
       "  '770',\n",
       "  '771',\n",
       "  '772',\n",
       "  '773',\n",
       "  '774',\n",
       "  '775',\n",
       "  '776',\n",
       "  '777',\n",
       "  '778',\n",
       "  '779',\n",
       "  '780',\n",
       "  '781',\n",
       "  '782',\n",
       "  '783',\n",
       "  '784',\n",
       "  '785',\n",
       "  '786',\n",
       "  '787',\n",
       "  '788',\n",
       "  '789',\n",
       "  '790',\n",
       "  '791',\n",
       "  '792',\n",
       "  '793',\n",
       "  '794',\n",
       "  ...],\n",
       " 'vect__strip_accents': None,\n",
       " 'vect__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       " 'vect__tokenizer': <function __main__.tokenize(text)>,\n",
       " 'vect__vocabulary': None,\n",
       " 'tfidf__norm': 'l2',\n",
       " 'tfidf__smooth_idf': True,\n",
       " 'tfidf__sublinear_tf': False,\n",
       " 'tfidf__use_idf': True,\n",
       " 'rfc__estimator__bootstrap': True,\n",
       " 'rfc__estimator__class_weight': None,\n",
       " 'rfc__estimator__criterion': 'gini',\n",
       " 'rfc__estimator__max_depth': None,\n",
       " 'rfc__estimator__max_features': 'auto',\n",
       " 'rfc__estimator__max_leaf_nodes': None,\n",
       " 'rfc__estimator__min_impurity_decrease': 0.0,\n",
       " 'rfc__estimator__min_impurity_split': None,\n",
       " 'rfc__estimator__min_samples_leaf': 1,\n",
       " 'rfc__estimator__min_samples_split': 2,\n",
       " 'rfc__estimator__min_weight_fraction_leaf': 0.0,\n",
       " 'rfc__estimator__n_estimators': 50,\n",
       " 'rfc__estimator__n_jobs': 10,\n",
       " 'rfc__estimator__oob_score': False,\n",
       " 'rfc__estimator__random_state': 4999,\n",
       " 'rfc__estimator__verbose': 0,\n",
       " 'rfc__estimator__warm_start': False,\n",
       " 'rfc__estimator': RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "             max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "             min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "             min_samples_leaf=1, min_samples_split=2,\n",
       "             min_weight_fraction_leaf=0.0, n_estimators=50, n_jobs=10,\n",
       "             oob_score=False, random_state=4999, verbose=0,\n",
       "             warm_start=False),\n",
       " 'rfc__n_jobs': 1}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc_pipe.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "#     'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "#     'vect__max_df': [0.8, 1.0],\n",
    "    'rfc__estimator__n_estimators': [30, 50]\n",
    "}\n",
    "\n",
    "cv = GridSearchCV(rfc_pipe, param_grid=parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rfc__estimator__n_estimators': 50}\n",
      "CPU times: user 18min 12s, sys: 8.97 s, total: 18min 21s\n",
      "Wall time: 3min 35s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "cv.fit(X_train, y_train)\n",
    "\n",
    "print(cv.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Test your model\n",
    "Show the accuracy, precision, and recall of the tuned model.  \n",
    "\n",
    "Since this project focuses on code quality, process, and  pipelines, there is no minimum performance metric needed to pass. However, make sure to fine tune your models for accuracy, precision and recall to make your project stand out - especially for your portfolio!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           Precision      Recall    F1_score     support\n",
      "                 related        0.81        0.81        0.81     4998.00\n",
      "                 request        0.89        0.89        0.89     1116.00\n",
      "                   offer        1.00        1.00        1.00       27.00\n",
      "             aid_related        0.78        0.78        0.78     2679.00\n",
      "            medical_help        0.92        0.92        0.92      530.00\n",
      "        medical_products        0.95        0.95        0.95      324.00\n",
      "       search_and_rescue        0.97        0.97        0.97      181.00\n",
      "                security        0.98        0.98        0.98      120.00\n",
      "                military        0.97        0.97        0.97      210.00\n",
      "             child_alone        1.00        1.00        1.00        0.00\n",
      "                   water        0.96        0.96        0.96      399.00\n",
      "                    food        0.95        0.95        0.95      702.00\n",
      "                 shelter        0.94        0.94        0.94      532.00\n",
      "                clothing        0.98        0.98        0.98      120.00\n",
      "                   money        0.98        0.98        0.98      156.00\n",
      "          missing_people        0.99        0.99        0.99       79.00\n",
      "                refugees        0.97        0.97        0.97      219.00\n",
      "                   death        0.96        0.96        0.96      288.00\n",
      "               other_aid        0.87        0.87        0.87      851.00\n",
      "  infrastructure_related        0.94        0.94        0.94      419.00\n",
      "               transport        0.95        0.95        0.95      322.00\n",
      "               buildings        0.96        0.96        0.96      309.00\n",
      "             electricity        0.98        0.98        0.98      137.00\n",
      "                   tools        0.99        0.99        0.99       40.00\n",
      "               hospitals        0.99        0.99        0.99       71.00\n",
      "                   shops        0.99        0.99        0.99       35.00\n",
      "             aid_centers        0.99        0.99        0.99       63.00\n",
      "    other_infrastructure        0.96        0.96        0.96      279.00\n",
      "         weather_related        0.88        0.88        0.88     1813.00\n",
      "                  floods        0.95        0.95        0.95      544.00\n",
      "                   storm        0.94        0.94        0.94      589.00\n",
      "                    fire        0.99        0.99        0.99       73.00\n",
      "              earthquake        0.97        0.97        0.97      614.00\n",
      "                    cold        0.98        0.98        0.98      127.00\n",
      "           other_weather        0.95        0.95        0.95      342.00\n",
      "           direct_report        0.85        0.85        0.85     1289.00\n"
     ]
    }
   ],
   "source": [
    "y_pred = cv.predict(X_test)\n",
    "multioutput_classification_report(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Try improving your model further. Here are a few ideas:\n",
    "* try other machine learning algorithms\n",
    "* add other features besides the TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'abc__estimator__learning_rate': 0.5}\n",
      "CPU times: user 14min 55s, sys: 5.78 s, total: 15min\n",
      "Wall time: 5min 16s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## adaboost pipeline\n",
    "abc_pipe = Pipeline([\n",
    "        ('vect', CountVectorizer(tokenizer=tokenize, stop_words=stopwords)),\n",
    "        ('tfidf', TfidfTransformer()),\n",
    "        ('abc', MultiOutputClassifier(AdaBoostClassifier(n_estimators=50, random_state=9982)))\n",
    "    ])\n",
    "\n",
    "## parameters for adaboost pipeline\n",
    "parameters_ada = {\n",
    "#     'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "#     'vect__max_df': [0.8, 1.0],\n",
    "#     'abc__estimator__n_estimators': [25, 50],\n",
    "    'abc__estimator__learning_rate': [0.5, 1]\n",
    "}\n",
    "\n",
    "cv_ada = GridSearchCV(abc_pipe, param_grid=parameters_ada)\n",
    "cv_ada.fit(X_train, y_train)\n",
    "print(cv_ada.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           Precision      Recall    F1_score     support\n",
      "                 related        0.76        0.76        0.76     4998.00\n",
      "                 request        0.89        0.89        0.89     1116.00\n",
      "                   offer        1.00        1.00        1.00       27.00\n",
      "             aid_related        0.75        0.75        0.75     2679.00\n",
      "            medical_help        0.93        0.93        0.93      530.00\n",
      "        medical_products        0.96        0.96        0.96      324.00\n",
      "       search_and_rescue        0.97        0.97        0.97      181.00\n",
      "                security        0.98        0.98        0.98      120.00\n",
      "                military        0.97        0.97        0.97      210.00\n",
      "             child_alone        1.00        1.00        1.00        0.00\n",
      "                   water        0.97        0.97        0.97      399.00\n",
      "                    food        0.95        0.95        0.95      702.00\n",
      "                 shelter        0.95        0.95        0.95      532.00\n",
      "                clothing        0.99        0.99        0.99      120.00\n",
      "                   money        0.98        0.98        0.98      156.00\n",
      "          missing_people        0.99        0.99        0.99       79.00\n",
      "                refugees        0.97        0.97        0.97      219.00\n",
      "                   death        0.97        0.97        0.97      288.00\n",
      "               other_aid        0.87        0.87        0.87      851.00\n",
      "  infrastructure_related        0.94        0.94        0.94      419.00\n",
      "               transport        0.96        0.96        0.96      322.00\n",
      "               buildings        0.96        0.96        0.96      309.00\n",
      "             electricity        0.98        0.98        0.98      137.00\n",
      "                   tools        0.99        0.99        0.99       40.00\n",
      "               hospitals        0.99        0.99        0.99       71.00\n",
      "                   shops        0.99        0.99        0.99       35.00\n",
      "             aid_centers        0.99        0.99        0.99       63.00\n",
      "    other_infrastructure        0.96        0.96        0.96      279.00\n",
      "         weather_related        0.87        0.87        0.87     1813.00\n",
      "                  floods        0.96        0.96        0.96      544.00\n",
      "                   storm        0.94        0.94        0.94      589.00\n",
      "                    fire        0.99        0.99        0.99       73.00\n",
      "              earthquake        0.97        0.97        0.97      614.00\n",
      "                    cold        0.98        0.98        0.98      127.00\n",
      "           other_weather        0.95        0.95        0.95      342.00\n",
      "           direct_report        0.85        0.85        0.85     1289.00\n"
     ]
    }
   ],
   "source": [
    "y_pred = cv_ada.predict(X_test)\n",
    "multioutput_classification_report(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Export your model as a pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('classifier.pkl', 'wb') as file:\n",
    "    pickle.dump(cv_ada, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Use this notebook to complete `train.py`\n",
    "Use the template file attached in the Resources folder to write a script that runs the steps above to create a database and export a model based on a new dataset specified by the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## tfidf matrix to achieve important words\n",
    "tfidf = TfidfVectorizer(min_df=0.01, max_df=0.90, tokenizer=tokenize, stop_words=stopwords)\n",
    "vect = CountVectorizer(tokenizer=tokenize, stop_words=stopwords)\n",
    "tfidf.fit(X)\n",
    "vect.fit(X)\n",
    "\n",
    "tfidf_vocab = tfidf.vocabulary_\n",
    "vect_vocab = vect.vocabulary_\n",
    "\n",
    "## make tfidf dataframe - word & rank\n",
    "tfidf_df = pd.DataFrame.from_dict(tfidf_vocab, orient='index').reset_index()\n",
    "tfidf_df.columns = ['word', 'rank']\n",
    "\n",
    "## make count vectorizer dataframe - word & count\n",
    "vect_df = pd.DataFrame.from_dict(vect_vocab, orient='index').reset_index()\n",
    "vect_df.columns = ['word', 'count']\n",
    "\n",
    "## merge two dataframe in the order of count\n",
    "vocab_df = pd.merge(tfidf_df, vect_df, how='left', on=['word']).sort_values('count', ascending=False).reset_index(drop=True)\n",
    "vocab_df = vocab_df[['word', 'count']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>year</td>\n",
       "      <td>31038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>would</td>\n",
       "      <td>30771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>world</td>\n",
       "      <td>30743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>working</td>\n",
       "      <td>30739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>work</td>\n",
       "      <td>30733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>woman</td>\n",
       "      <td>30699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>without</td>\n",
       "      <td>30659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>west</td>\n",
       "      <td>30423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>well</td>\n",
       "      <td>30388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>week</td>\n",
       "      <td>30361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>weather</td>\n",
       "      <td>30339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>way</td>\n",
       "      <td>30301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>water</td>\n",
       "      <td>30258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>want</td>\n",
       "      <td>30180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>village</td>\n",
       "      <td>29893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>victim</td>\n",
       "      <td>29840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>united</td>\n",
       "      <td>29285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>un</td>\n",
       "      <td>29042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>u</td>\n",
       "      <td>28835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>two</td>\n",
       "      <td>28809</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       word  count\n",
       "0      year  31038\n",
       "1     would  30771\n",
       "2     world  30743\n",
       "3   working  30739\n",
       "4      work  30733\n",
       "5     woman  30699\n",
       "6   without  30659\n",
       "7      west  30423\n",
       "8      well  30388\n",
       "9      week  30361\n",
       "10  weather  30339\n",
       "11      way  30301\n",
       "12    water  30258\n",
       "13     want  30180\n",
       "14  village  29893\n",
       "15   victim  29840\n",
       "16   united  29285\n",
       "17       un  29042\n",
       "18        u  28835\n",
       "19      two  28809"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
